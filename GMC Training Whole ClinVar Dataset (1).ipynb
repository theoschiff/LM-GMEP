{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e05ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0829b9",
   "metadata": {},
   "source": [
    "### Reading hgvs4variation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb95878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hgvs_var_wd = pd.read_csv(\"hgvs4variation.txt\", delimiter = '\\t')\n",
    "hgvs_var_wd.iloc[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24793fd9",
   "metadata": {},
   "source": [
    "### Reading variant_summary.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b92d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_summ_wd = pd.read_csv('variant_summary.txt', delimiter = '\\t')\n",
    "variant_summ_wd.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5848181",
   "metadata": {},
   "source": [
    "# DICTIONNARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66792c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "daatol = {\n",
    "    \"Cys\": \"C\",\n",
    "    \"Asp\": \"D\",\n",
    "    \"Ser\": \"S\",\n",
    "    \"Gln\": \"Q\",\n",
    "    \"Lys\": \"K\",\n",
    "    \"Ile\": \"I\",\n",
    "    \"Pro\": \"P\",\n",
    "    \"Thr\": \"T\",\n",
    "    \"Phe\": \"F\",\n",
    "    \"Asn\": \"N\",\n",
    "    \"Gly\": \"G\",\n",
    "    \"His\": \"H\",\n",
    "    \"Leu\": \"L\",\n",
    "    \"Arg\": \"R\",\n",
    "    \"Trp\": \"W\",\n",
    "    \"Ala\": \"A\",\n",
    "    \"Val\": \"V\",\n",
    "    \"Glu\": \"E\",\n",
    "    \"Tyr\": \"Y\",\n",
    "    \"Met\": \"M\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efebc6cb",
   "metadata": {},
   "source": [
    "Text file with all genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.expanduser(\"~/MIT/gene_list.txt\"), 'w') as f:\n",
    "            f.write('')\n",
    "for g in dic:\n",
    "    with open(os.path.expanduser(\"~/MIT/gene_list.txt\"), 'a') as f:\n",
    "                            f.write(f\"{g}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe3702",
   "metadata": {},
   "source": [
    "## Dic with all sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c939927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b9d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biotite.sequence.io.fasta import FastaFile, get_sequences\n",
    "seq_dic = {}\n",
    "infile = FastaFile()\n",
    "infile.read(os.path.expanduser(\"~/MIT/clinvar_genes_seq.fasta\"))\n",
    "#header = infile.split('GN=')[1].split(' ')[0]\n",
    "seqs = get_sequences(infile)\n",
    "for header, seq in tqdm(seqs.items()):\n",
    "    #items c'est le bon truc\n",
    "    gene = header.split('GN=')[1].split(' ')[0]\n",
    "    seq_dic[gene] = str(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb1349",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_genes = [g for g in seq_dic]\n",
    "effective_genes = set(effective_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79251017",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_names = set([n for n in hgvs_var_wd[\"Symbol\"]  if n != 'TERC' ])\n",
    "dic = {gene: {} for gene in effective_genes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002201ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab5f5bf9",
   "metadata": {},
   "source": [
    "## Dic of dic of dic: (Gene) Symbol: {AlleleID : {Protein Change : Clinical Significance}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4668b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "regex = re.compile('^[A-Za-z]{3}[0-9]+[A-Za-z]{3}$')\n",
    "unwanted_mut = {'Ter', 'del', 'Del', 'DEL', 'dup', 'TER', 'Xaa'}\n",
    "for row in hgvs_var_wd.itertuples():\n",
    "    if (row.Symbol in effective_genes):\n",
    "    #exclude alleles that do not make a change in protein\n",
    "        if (row.ProteinChange != '-'):\n",
    "            proteinChange = row.ProteinChange.split('.')[1]\n",
    "            #make sure proteinChange matches the format 3 letters + number + 3 letters\n",
    "            if (regex.match(proteinChange)):\n",
    "                if (proteinChange[:3] not in unwanted_mut) and (proteinChange[-3:] not in unwanted_mut):\n",
    "                    loc = int(proteinChange[3:-3])\n",
    "                    #19/08 added the condition to only keep the genes that are compatible with alphafold\n",
    "                    #if(loc < 1022):\n",
    "                    if(len(seq_dic[row.Symbol]) > loc-1):\n",
    "                        wt = proteinChange[:3]\n",
    "                        wt = wt[0].upper() + wt[1:].lower()\n",
    "                        #voir si on garde Ã§a exactement ou si on change cette condition\n",
    "                        if(wt in daatol and daatol[wt] == seq_dic[row.Symbol][loc-1]):\n",
    "                                dic[row.Symbol][row.AlleleID] = {\"Change\" : proteinChange}\n",
    "for row in variant_summ_wd.itertuples():\n",
    "    if (row.GeneSymbol in dic):\n",
    "        if (row.AlleleID in dic[row.GeneSymbol]):\n",
    "            if (row.ClinicalSignificance is not None):\n",
    "                dic[row.GeneSymbol][row.AlleleID][\"ClinicalSignificance\"] = row.ClinicalSignificance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathogenic = {'Pathogenic','Pathogenic/Likely pathogenic','Pathogenic/Likely pathogenic; risk factor',\n",
    "                  'Likely pathogenic', 'risk factor',\n",
    "                  'Likely pathogenic; Affects',\n",
    "                  'Likely pathogenic; association',\n",
    "                  'Likely pathogenic; drug response',\n",
    "                  'Likely pathogenic; other',\n",
    "                  'Likely pathogenic; risk factor',\n",
    "                  'Likely risk allele',\n",
    "                  'Pathogenic/Likely pathogenic; drug response',\n",
    "                  'Pathogenic/Likely pathogenic; other',\n",
    "                  'Pathogenic/Likely risk allele',\n",
    "                  'Pathogenic; Affects',\n",
    "                  'Pathogenic; association',\n",
    "                  'Pathogenic; drug response',\n",
    "                  'Pathogenic; drug response; other',\n",
    "                  'Pathogenic; other',\n",
    "                  'Pathogenic; protective',\n",
    "                  'Pathogenic; risk factor'}\n",
    "benign = {'Benign', 'Benign/Likely benign', 'Likely benign',\n",
    "            'Benign/Likely benign; association',\n",
    "            'Benign/Likely benign; drug response',\n",
    "            'Benign/Likely benign; drug response; other',\n",
    "            'Benign/Likely benign; other',\n",
    "            'Benign/Likely benign; other; risk factor',\n",
    "            'Benign/Likely benign; risk factor',\n",
    "            'Benign; Affects',\n",
    "            'Benign; association',\n",
    "            'Benign; confers sensitivity',\n",
    "            'Benign; drug response',\n",
    "            'Benign; other',\n",
    "            'Benign; protective',\n",
    "            'Benign; risk factor',\n",
    "            'Likely benign; Affects',\n",
    "            'Likely benign; drug response',\n",
    "            'Likely benign; drug response; other',\n",
    "            'Likely benign; other',\n",
    "            'Likely benign; risk factor'}\n",
    "vus = {'Uncertain significance', 'Conflicting interpretations of pathogenicity',\n",
    "          'Affects',\n",
    "          'Affects; association',\n",
    "          'Affects; risk factor',\n",
    "          'Conflicting interpretations of pathogenicity; Affects',\n",
    "          'Conflicting interpretations of pathogenicity; association',\n",
    "          'Conflicting interpretations of pathogenicity; association; risk factor',\n",
    "          'Conflicting interpretations of pathogenicity; drug response',\n",
    "          'Conflicting interpretations of pathogenicity; other',\n",
    "          'Conflicting interpretations of pathogenicity; other; risk factor',\n",
    "          'Conflicting interpretations of pathogenicity; risk factor',\n",
    "          'Uncertain significance; Affects',\n",
    "          'Uncertain significance; association',\n",
    "          'Uncertain significance; drug response',\n",
    "          'Uncertain significance; other',\n",
    "          'Uncertain significance; risk factor',\n",
    "          'association',\n",
    "          'confers sensitivity',\n",
    "          'drug response',\n",
    "          'drug response; other',\n",
    "          'drug response; risk factor',\n",
    "          'no interpretation for the single variant',\n",
    "          'not provided',\n",
    "          'other',\n",
    "          'protective',\n",
    "          'protective; risk factor'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74135592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "num_data = 0\n",
    "#faut-il que je le fasse plutÃ´t comme Ã§a pour le dico gÃ©nÃ©ral?\n",
    "pathogenicity = collections.defaultdict(dict)\n",
    "for g in dic:\n",
    "    num_patho = 0\n",
    "    num_benign = 0\n",
    "    num_vus = 0\n",
    "    for a in dic[g]:\n",
    "        wild_type = dic[g][a]['Change'][:3] \n",
    "        loc = int(dic[g][a]['Change'][3 : -3])\n",
    "        #print(dic[g][a])\n",
    "        #pas compris pourquoi ce if n'est pas solved par le filtre du dico\n",
    "        if('ClinicalSignificance' in dic[g][a].keys()):\n",
    "            num_data +=1\n",
    "            if(dic[g][a]['ClinicalSignificance'] in pathogenic):\n",
    "                num_patho += 1\n",
    "                pathogenicity[g]['Num of Pathogenic'] = num_patho\n",
    "\n",
    "            if(dic[g][a]['ClinicalSignificance'] in benign):\n",
    "                num_benign += 1\n",
    "                pathogenicity[g]['Num of Benign'] = num_benign\n",
    "\n",
    "            if(dic[g][a]['ClinicalSignificance'] in vus): \n",
    "                num_vus += 1\n",
    "                pathogenicity[g]['Num of VUS'] = num_vus\n",
    "            if(num_patho == 0):\n",
    "                   pathogenicity[g]['Num of Pathogenic'] = num_patho\n",
    "            if(num_benign == 0):\n",
    "                pathogenicity[g]['Num of Benign'] = num_benign\n",
    "            if(num_vus == 0):\n",
    "                pathogenicity[g]['Num of VUS'] = num_vus\n",
    "                \n",
    "print(\"number of filtered data\", num_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cae4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pathogenicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c2b203",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45afaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(sum(pathogenicity[ge]['Num of VUS'] for ge in pathogenicity), \"VUSs\")\n",
    "print(sum(pathogenicity[ge]['Num of Pathogenic'] for ge in pathogenicity), 'Pathogenic')\n",
    "print(sum(pathogenicity[ge]['Num of Benign'] for ge in pathogenicity), 'Benign')\n",
    "\n",
    "X = np.arange(len(pathogenicity))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.set_title('Distribution of Pathogenicity per gene')\n",
    "ax.set_ylabel('Number of cases')\n",
    "\n",
    "ax.bar(X + 0.00, [pathogenicity[ge]['Num of VUS'] for ge in pathogenicity], color ='#fcff42', width = 0.25)\n",
    "ax.bar(X + 0.25, [pathogenicity[ge]['Num of Pathogenic'] for ge in pathogenicity], color = '#426eff', width = 0.25)\n",
    "ax.bar(X + 0.50, [pathogenicity[ge]['Num of Benign'] for ge in pathogenicity], color = 'r', width = 0.25)\n",
    "plt.legend(['VUS','Benign', 'Pathogenic'])   \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7306875c",
   "metadata": {},
   "source": [
    "## PREPROCESS FOR LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489dcbc4",
   "metadata": {},
   "source": [
    "### Padding sequences to feed them to model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50dc845",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_daatol = [daatol[aa] for aa in daatol]\n",
    "ordered_daatol.sort()\n",
    "x_alphabet = {}\n",
    "x_alphabet['<PAD>'] = 0\n",
    "i = 1\n",
    "for aa in ordered_daatol:\n",
    "    x_alphabet[aa] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79421e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_alphabet = {'Benign' : 0, 'Pathogenic' : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_seq_dic = {gene : {} for gene in dic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dde758",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_seq_dic = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7fe01e",
   "metadata": {},
   "source": [
    "## Create a dictionnary with the mutated sequence and mutation centered if length > 1023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df70d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1023\n",
    "for g in dic:\n",
    "    for a in dic[g]:\n",
    "        mutation = dic[g][a]['Change'][-3:] \n",
    "        loc = int(dic[g][a]['Change'][3 : -3])\n",
    "        if (mutation != 'dup'):\n",
    "            if ('ClinicalSignificance' in dic[g][a]):\n",
    "                if(dic[g][a]['ClinicalSignificance'] not in vus):\n",
    "                    aa = daatol[mutation]\n",
    "                    l_mutated_seq = list(seq_dic[g])\n",
    "                    l_mutated_seq[loc - 1] = aa\n",
    "                    if(loc > 1023):\n",
    "                        shifted_seq_dic[g] = seq_dic[g][max(0, loc - 1 - max_length // 2) : min(loc - 1 + max_length // 2, len(l_mutated_seq))]\n",
    "                        l_mutated_seq = l_mutated_seq[max(0, loc - 1 - max_length // 2) : min(loc - 1 + max_length // 2, len(l_mutated_seq))]\n",
    "                    else:\n",
    "                        shifted_seq_dic[g] = seq_dic[g][:max_length]\n",
    "                        l_mutated_seq = l_mutated_seq[:max_length]\n",
    "                    mutated_seq = \"\".join([str(i) for i in l_mutated_seq])\n",
    "                    mut_seq_dic[g][a] = mutated_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data, x_true_data = {gene : {} for gene in dic}, {gene : {} for gene in dic}, {gene : {} for gene in dic}\n",
    "for g in mut_seq_dic:\n",
    "    if (not mut_seq_dic[g]):\n",
    "        continue\n",
    "    for a in mut_seq_dic[g]:\n",
    "        x_true_data[g] = [x_alphabet[letter] for letter in shifted_seq_dic[g]]\n",
    "        x_data[g][a] = [x_alphabet[letter] for letter in mut_seq_dic[g][a]]\n",
    "        if dic[g][a]['ClinicalSignificance'] in benign:\n",
    "            y_data[g][a] = y_alphabet['Benign']\n",
    "        if dic[g][a]['ClinicalSignificance'] in pathogenic:        \n",
    "            y_data[g][a] = y_alphabet['Pathogenic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae39848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(y_data), len(x_data), len(x_true_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532eb6ab",
   "metadata": {},
   "source": [
    "## Split training, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b02f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "# Iterating through the json\n",
    "# list\n",
    "x_train , x_true_train, x_valid, x_true_valid, x_test, x_true_test, y_train, y_valid, y_test = [],[],[],[],[],[],[],[],[]\n",
    "# Opening JSON file\n",
    "f = open(os.path.expanduser(\"~/MIT/split.json\"))\n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "for g in x_data:\n",
    "    for a in x_data[g]:\n",
    "        if (data[g] == 'train'):\n",
    "            x_train.append(x_data[g][a])\n",
    "            x_true_train.append(x_true_data[g])\n",
    "            y_train.append(y_data[g][a])\n",
    "        if (data[g] == 'dev'):\n",
    "            x_valid.append(x_data[g][a])\n",
    "            x_true_valid.append(x_true_data[g])\n",
    "            y_valid.append(y_data[g][a])\n",
    "        else:\n",
    "            x_test.append(x_data[g][a])\n",
    "            x_true_test.append(x_true_data[g])\n",
    "            y_test.append(y_data[g][a])\n",
    "\n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b1aa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(x_train), \"xtrain\\n\", len(x_valid), 'xvalid\\n', len(x_test), 'xtest\\n',len(y_train), 'ytrain\\n', len(y_valid), 'yvalid\\n', \n",
    "     len(y_test), 'ytest\\n', len(x_true_train), 'xtrue train\\n', len(x_true_valid), 'xtrue valid\\n', len(x_true_test), 'xtrue test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a63436",
   "metadata": {},
   "source": [
    "## Pad Sequences for train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d412e7e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_lengths = [len(sequence) for sequence in x_train]\n",
    "# create an empty matrix with padding tokens\n",
    "pad_token = x_alphabet['<PAD>']\n",
    "longest_seq = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X_train = np.ones((len(x_train), longest_seq)) * pad_token\n",
    "#copy over the actual sequences\n",
    "for i, x_len in enumerate(X_lengths):\n",
    "        sequence = x_train[i]\n",
    "        padded_X_train[i, 0:x_len] = sequence[:x_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f610f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lengths_valid = [len(sequence) for sequence in x_valid]\n",
    "# create an empty matrix with padding tokens\n",
    "padded_X_valid = np.ones((len(x_valid), longest_seq)) * pad_token\n",
    "#copy over the actual sequences\n",
    "for i, x_len in enumerate(X_lengths_valid):\n",
    "        sequence = x_valid[i]\n",
    "        padded_X_valid[i, 0:x_len] = sequence[:x_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549dcd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lengths_test = [len(sequence) for sequence in x_test]\n",
    "# create an empty matrix with padding tokens\n",
    "padded_X_test = np.ones((len(x_test), longest_seq)) * pad_token\n",
    "#copy over the actual sequences\n",
    "for i, x_len in enumerate(X_lengths_test):\n",
    "        sequence = x_test[i]\n",
    "        padded_X_test[i, 0:x_len] = sequence[:x_len]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec3721",
   "metadata": {},
   "source": [
    "## Padding for true sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35934dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true_lengths =  [len(sequence) for sequence in x_true_train]\n",
    "padded_X_true_train = np.ones((len(x_true_train), longest_seq)) * pad_token\n",
    "for i, x_len in enumerate(X_true_lengths):\n",
    "    sequence = x_true_train[i]\n",
    "    padded_X_true_train[i, 0:x_len] = sequence[:x_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true_lengths_valid = [len(sequence) for sequence in x_true_valid]\n",
    "# create an empty matrix with padding tokens\n",
    "padded_X_true_valid = np.ones((len(x_true_valid), longest_seq)) * pad_token\n",
    "#copy over the actual sequences\n",
    "for i, x_len in enumerate(X_true_lengths_valid):\n",
    "        sequence = x_true_valid[i]\n",
    "        padded_X_true_valid[i, 0:x_len] = sequence[:x_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true_lengths_test = [len(sequence) for sequence in x_true_test]\n",
    "# create an empty matrix with padding tokens\n",
    "padded_X_true_test = np.ones((len(x_true_test), longest_seq)) * pad_token\n",
    "#copy over the actual sequences\n",
    "for i, x_len in enumerate(X_true_lengths_test):\n",
    "        sequence = x_true_test[i]\n",
    "        padded_X_true_test[i, 0:x_len] = sequence[:x_len]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f806cdc",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368aa00",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3859b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafbad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AaSeqClassifier(nn.Module):\n",
    "    def __init__(self,input_size, hidden_dim, dropout, LSTM_layers, batch_size, use_transf = False, use_LM = False):\n",
    "        super(AaSeqClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.LSTM_layers = LSTM_layers\n",
    "        self.input_size = input_size\n",
    "        self.use_transf = use_transf\n",
    "        #nn.Dropout: During training, randomly zeroes some of the elements of the input tensor\n",
    "        #with probability p using samples from a Bernoulli distribution.\n",
    "        #Each channel will be zeroed out independently on every forward call. \n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n",
    "        #if (not use_transf):\n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim * 2,\n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.LSTM_layers,\n",
    "                            batch_first=True)\n",
    "#        else: \n",
    "#            self.transformer = Encoder(self.hidden_dim, LSTM_layers, 2, 2, dropout, longest_seq)\n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_dim,\n",
    "                             out_features=self.hidden_dim*2)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim*2, len(y_alphabet))\n",
    "\n",
    "    def forward(self, x, x_true, lengths, cuda: bool = False):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch. Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "\n",
    "#        if cuda:\n",
    "#            lengths = lengths.detach().cpu().numpy()\n",
    "#        else: \n",
    "#            lenghts = lengths.detach().numpy()\n",
    "        if(self.use_LM):\n",
    "            ...\n",
    "        else:\n",
    "            pad_mask = x != 0\n",
    "            lenghts = lengths.detach().cpu().numpy()\n",
    "            out = self.embedding(x)\n",
    "            out_true = self.embedding(x_true)\n",
    "            out = torch.cat((out, out - out_true), dim=-1)\n",
    "            #if (not self.use_transf):\n",
    "\n",
    "            # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "            out = torch.nn.utils.rnn.pack_padded_sequence(out, lengths.detach().cpu().numpy(), batch_first=True, enforce_sorted = False)\n",
    "            # Feed LSTMs\n",
    "            out, (hidden, cell) = self.lstm(out)\n",
    "            out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "    #        else:\n",
    "    #            out = self.transformer(out, mask = x != x_alphabet['<PAD>'])\n",
    "            out = self.dropout(out)\n",
    "\n",
    "            # The last hidden state is taken\n",
    "            #attention au padding avec -1\n",
    "            out = (out * pad_mask.unsqueeze(2)).sum(dim=1) / pad_mask.sum(dim=1, keepdim=True)\n",
    "            out = torch.relu_(self.fc1(out))\n",
    "            out = self.dropout(out)\n",
    "            out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27c8c9",
   "metadata": {},
   "source": [
    "### prepare the way how the sequences will be fed to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d7ac51",
   "metadata": {},
   "source": [
    "### For LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.data import Dataset\n",
    "#from torch.utils.data import DataLoader\n",
    "\n",
    "#class DatasetMaper(Dataset):\n",
    "#    '''Handles batches of dataset'''\n",
    "#  \n",
    "#    def __init__(self, x, x_true, y, lengths):\n",
    "#        self.x = x\n",
    "#        self.x_true = x_true\n",
    "#        self.y = y\n",
    "#        self.lengths = lengths\n",
    "\n",
    "#    def __len__(self):\n",
    "#        return len(self.x)\n",
    "#    \n",
    "#    def __getitem__(self, idx):\n",
    "#        return self.x[idx], self.x_true[idx], self.y[idx], self.lengths[idx]\n",
    "\n",
    "#train_batch_size = 128\n",
    "#batch_size = 512\n",
    "#training_set = DatasetMaper(padded_X_train, padded_X_true_train, y_train, X_lengths)\n",
    "#validation_set = DatasetMaper(padded_X_valid, padded_X_true_valid, y_valid, X_lengths_valid)\n",
    "#test_set = DatasetMaper(padded_X_test, padded_X_true_test, y_test, X_lengths_test)\n",
    "#loader_training = DataLoader(training_set, batch_size = train_batch_size, shuffle=True, num_workers=0)\n",
    "#loader_valid = DataLoader(validation_set, batch_size = batch_size)\n",
    "#loader_test = DataLoader(test_set, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505e2ef",
   "metadata": {},
   "source": [
    "## FOR Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a828710",
   "metadata": {},
   "source": [
    "### Data Processing ESM-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d4f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_esm2 = []\n",
    "for g in mut_seq_dic:\n",
    "    for a in mut_seq_dic[g]:\n",
    "        data_esm2.append((f\"{g}_{a}_wt\", shifted_seq_dic[g]))\n",
    "        data_esm2.append((f\"{g}_{a}_mut\", mut_seq_dic[g][a]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ede969",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_esm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DatasetMaper(Dataset):\n",
    "    '''Handles batches of dataset'''\n",
    "  \n",
    "    def __init__(self, x, x_true, y, lengths, number, mask: bool = false):\n",
    "        self.x = x\n",
    "        self.x_true = x_true\n",
    "        self.y = y\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.x_true[idx], self.y[idx], self.lengths[idx]\n",
    "\n",
    "train_batch_size = 128\n",
    "batch_size = 512\n",
    "training_set = DatasetMaper(padded_X_train, padded_X_true_train, y_train, X_lengths)\n",
    "validation_set = DatasetMaper(padded_X_valid, padded_X_true_valid, y_valid, X_lengths_valid)\n",
    "test_set = DatasetMaper(padded_X_test, padded_X_true_test, y_test, X_lengths_test)\n",
    "loader_training = DataLoader(training_set, batch_size = train_batch_size, shuffle=True, num_workers=0)\n",
    "loader_valid = DataLoader(validation_set, batch_size = batch_size)\n",
    "\n",
    "loader_test = DataLoader(test_set, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee878b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddc71189",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6387cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics.accuracy_score(y_pred.cpu().detach().numpy(), y.cpu().detach().numpy())), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62660b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    correct = (preds == y)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, optimizer, dataloader_train, dataloader_test, epochs):\n",
    "    #initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "  # model in training mode\n",
    "    model.train()\n",
    "\n",
    "    #x_batch, y_batch = next(iter(loader_training))\n",
    "    for x_batch, x_true_batch, y_batch, lengths in tqdm(loader_training):\n",
    "\n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = torch.tensor(x_batch).to(torch.long)\n",
    "        x_true = torch.tensor(x_true_batch).to(torch.long)\n",
    "        y = torch.tensor(y_batch).to(torch.long)\n",
    "        lengths = torch.tensor(lengths).to(torch.long)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            x_true = x_true.cuda()\n",
    "            y = y.cuda()\n",
    "            lengths = lengths.cuda()\n",
    "\n",
    "        # Feed the model and get output \"y_pred\"\n",
    "        y_pred = model(x, x_true, lengths)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        #compute the binary accuracy\n",
    "        #acc = binary_accuracy(torch.argmax(y_pred, dim=1), y)\n",
    "\n",
    "        # The gradients are calculated\n",
    "        #backpropage the loss and compute the gradients\n",
    "        # i.e. derivatives are calculated\n",
    "        loss.backward()\n",
    "\n",
    "        # Each parameter is updated\n",
    "        # with torch.no_grad():\n",
    "        #     a -= lr * a.grad\n",
    "        #     b -= lr * b.grad\n",
    "        optimizer.step()\n",
    "        \n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()  \n",
    "        #epoch_acc += acc.item()    \n",
    "        \n",
    "    return epoch_loss / len(loader_training)#, epoch_acc / len(loader_training)\n",
    "        \n",
    "#        print('\\rEp {}/{}, loss train: {:.2f}, \n",
    "#                  format(epoch + 1, epochs, len(loader_training), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377c45f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d59f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation():\n",
    "\n",
    "    predictions = []\n",
    "    y_true = []\n",
    "    \n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_auc = 0\n",
    "    #model is turned in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Skipping gradients update\n",
    "    #Disabling gradient calculation is useful for inference,\n",
    "    #when you are sure that you will not call Tensor.backward().\n",
    "    #It will reduce memory consumption for computations that would otherwise have requires_grad=True\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for x_batch, x_true_batch, y_batch, lengths in tqdm(loader_valid):\n",
    "        \n",
    "            x = torch.tensor(x_batch).to(torch.long)\n",
    "            x_true = torch.tensor(x_true_batch).to(torch.long)\n",
    "            y = torch.tensor(y_batch).to(torch.long)\n",
    "            lengths = torch.tensor(lengths).to(torch.long)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "                x_true = x_true.cuda()\n",
    "                y = y.cuda()\n",
    "                lengths = lengths.cuda()\n",
    "\n",
    "        \n",
    "            y_pred = model(x, x_true, lengths)\n",
    "        \n",
    "            predictions += list(y_pred.detach().cpu().numpy())\n",
    "            y_true += list(y.detach().cpu().numpy())\n",
    "            \n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            acc = binary_accuracy(torch.argmax(y_pred, dim = 1).detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.cpu().item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    auc = roc_auc_score(torch.FloatTensor(y_true).numpy(), torch.softmax(torch.FloatTensor(predictions), dim=1)[:, 1])\n",
    "    return epoch_loss / len(loader_valid), epoch_acc / len(loader_valid), auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7065927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path= os.path.expanduser('~/MIT/model_eval/saved_weights.pt')\n",
    "hidden_dim, dropout, LSTM_layers, batch_size, epochs = 256, 0.0, 2, 128, 20\n",
    "model = AaSeqClassifier(len(x_alphabet),hidden_dim, dropout, LSTM_layers,batch_size, epochs)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(\"Transferred model to GPU\")\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "print(model)\n",
    "print(f\"The model has {sum([x.nelement() for x in model.parameters()]):,} parameters.\")\n",
    "best_valid_loss = float('inf')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "     \n",
    "    #train the model\n",
    "    train_loss = train_model(model, optimizer, loader_training, criterion, epochs)\n",
    "    \n",
    "    #evaluate the model\n",
    "    valid_loss, valid_acc, valid_auc = evaluation()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), path)\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Val. AUC: {valid_auc*100:.2f}')\n",
    "    \n",
    "    #Erreur : Expected all tensors to be on the same device, but found at least two devices,\n",
    "    #cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af8009",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model.load_state_dict(torch.load(path));\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea573f",
   "metadata": {},
   "source": [
    "# ESM-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758aa106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # IMPROTANT: Path a un folder de ton choix sur ton ordi\n",
    "    torch.hub.set_dir(\"/data/scratch/schifthe\")\n",
    "    \n",
    "    # Load ESM-2 model\n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()  # disables dropout for deterministic results\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        print(\"Transferred model to GPU\")\n",
    "        \n",
    "    results_esm_1v = {gene: {} for gene in shift_seq_dic}\n",
    "    #faire une loop et prendre deux par deux et appeler le batch converter dessus\n",
    "    for g in tqdm(dic):\n",
    "        for a in tqdm(dic[g]):\n",
    "            \n",
    "    batch_labels, _, batch_tokens = batch_converter(data_esm2)\n",
    "    \n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][33]\n",
    "\n",
    "    # Generate per-sequence representations via averaging\n",
    "    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "    sequence_representations = []\n",
    "    for i, (_, seq) in tqdm(enumerate(data_esm2)):\n",
    "        sequence_representations.append(token_representations[i, 1 : len(seq) + 1].mean(0))\n",
    "\n",
    "\n",
    "    # Look at the unsupervised self-attention map contact predictions\n",
    "    import matplotlib.pyplot as plt\n",
    "    for (_, seq), attention_contacts in zip(data_esm2, results[\"contacts\"]):\n",
    "        plt.matshow(attention_contacts[: len(seq), : len(seq)])\n",
    "        plt.title(seq)\n",
    "        plt.show()\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    results_esm2 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4104561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "global_scores = [torch.sigmoid(torch.tensor(results_esm_1v[g][a])).item() for g in results_esm_1v for a in results_esm_1v[g]]\n",
    "global_labels = [dic[g][a][\"ClinicalSignificance\"] in benign for g in results_esm_1v for a in results_esm_1v[g]]\n",
    "\n",
    "print(roc_auc_score(global_labels, global_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
